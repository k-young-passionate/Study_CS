# 워크로드 API 카테고리

## 워크로드 API 카테고리

### 쿠버네티스 리소스 카테고리

- workload API category: container 실행 관련 resource
- service API category: endpoint 제공하는 resource
- config & storage API category: 설정/기밀/persistent volume 등에 관련된 resource
- cluster API category: security, quater 등에 관련된 resource
- metadata API categoroy: cluster 내부 다른 resource를 관리하기 위한 resource

### workload API category

- Pod
- Replication controller
- ReplicaSet
- Deployment
- DaemonSet
- StatefulSet
- Job
- Cronjob

## Pod

- workload resource의 최소 단위
- 한 개 이상의 container로 구성
  - container끼리 하나의 ip주소 공유
- subcontainer
  - proxy 역할을 하는 container
  - 설정 값을 동적으로 변경시키는 container
  - local cache용 container
  - SSL용 container

### pod design pattern

#### sidecar pattern

- main container에 기능을 추가하는 방식
- ex) 변경사항 감시 => 동적으로 설정 변경 container

#### ambassador pattern

- 외부 system과의 통신 중계
- main container는 sub container와의 연결만 함
- 다른 여러 system과의 연결은 sub container가 중계
- 외부 통신에 대한 결합도 낮춤

#### adapter pattern

- 외부 접속을 위한 interface 제공
- 서로 다른 데이터 형식을 변환해줌

### pod 생성

- sample pod 
  ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: sample-pod
  spec:
    containers:
      - name: nginx-container
        image: nginx:1.16
      - name: redis-container
        image: redis:3.2
  ```
- 생성
  ```bash
  $ kubectl apply -f sample-pod.yaml # pod 생성
  $ kubectl get pods # pod 목록 표시
  $ kubectl get pods --outer wide # 상세한 resource 확인
  ```
- 같은 pod에서 port 사용하는 container 두 개 이상 생성할 경우, 하나만 작동

### container login & command execute

```bash
$ kubectl exec -it sample-pod -- /bin/bash
```

### command/args

- ENTRYPOINT / CMD 명령어 관계
  ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: sample-pod
  spec:
    containers:
      - name: nginx-container
        image: nginx:1.16
        command: ["/bin/sleep"]
        args: ["3600"]
  ```

### pod명 제한

- 영문 소문자, 숫자
- '-', '.'
- 시작/끝은 영문 소문자

### host network 구성을 사용한 Pod 기동

- `spec.hostNetwork` 에 설정
- network 설정으로 host상에서 process를 기동하는 것과 같은 network 구성으로 pod 기동 가능

  ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: sample-pod
  spec:
    hostNetwork: true # node의 ip 주소와 같은 ip주소 사용
    containers:
      - name: nginx-container
        image: nginx:1.16
  ```

- ip 주소 확인
  ```bash
  $ kubectl get pod sample-hostnetwork -o wide # pod ip주소 확인
  $ kubectl get node node-name -o wide # node ip주소 확인
  $ kubectl exect -it sample-hostnetwork -- hostname # pod의 host명 확인
  $ kubectl exect -it sample-hostnetwork -- cat /etc/resolv.conf # pod의 DNS 설정 확인
  ```

### pod DNS 설정과 service discovery

- `spec.dnsPolicy` 에 설정
- 설정값 4가지 존재

#### ClusterFirst

- cluster 내부 DNS에 질의 => 해석 안되면 upstream에 질의

#### None

- pod 정의 내에서 정적으로 설정
- dnsConfig에 설정하고 싶은 값 작성

```yaml
...
spec:
  dnsPolicy: None
  dnsConfig:
    nameservers:
    - 8.8.8.8
    - 8.8.4.4
    searches:
    - example.com
    options:
    - name: ndots
      value: "5"
...
```

#### Default

- kubernetes node의 /etc/resolv.conf 상속

#### ClusterFirstWithHostNet

- ClusterFirst 동작과 같음 (hostNetwork 사용시 설정)

### 정적 host명 해석 설정: /etc/hosts

- `spec.hostAliases` 로 설정

```yaml
...
spec:
  hostAliases:
  - ip: 8.8.8.8
    hostnames:
      - google-dns
      - google-public-dns
...
```

### 작업 디렉터리 설정

- docker file의 `WORKDIR` 설정 따름
- `spec.containers[].workingDir`로 덮어 쓸 수 있음

```yaml
...
spec:
  container:
  - name: nginx-container
    image: nginx:1.16
    workingDir: /tmp
...
```

## ReplicaSet/Replication Controller

- pod의 replica를 생성하고 지정한 pod 수를 유지하는 resource
- replication controller에서 ReplicaSet으로 이름 변경 + 기능 추가

### ReplicaSet 생성

```yaml
apiVersion: v1
kind: ReplicaSet
metadata:
  name: sample-rs
spec:
  replicas: 3
  selector:
    matchLabels:
      app: sample-app
  template:
    metadata:
      labels:
        app: sample-app
    spec:
      containers:
        - name: nginx-container
          image: nginx:1.16
```
- ReplicaSet이 생성하는 pod의 이름은 `ReplicaSet 이름`-`임의의 문자열` 로 구성

### pod 정지와 자동화된 복구

- pod 정지
  ```bash
  $ kubectl delete pod sample-rs-9f9kr
  ```
- ReplicaSet에 의해 다시 생성됨
- pod 수 증감 이력 확인
  ```bash
  $ kubectl describe ReplicaSet sample-rs
  ```

### RS와 label

- ReplicaSet은 kubernetes가 pod을 monitoring하여 pod 수 조정
- monitoring은 특정 label 가진 pod 수를 계산하는 형태로 이루어짐
- `spec.selector.matchLabels.app`에 있는 label 추적
- `spec.template.metadata.label`에 있는 값대로 pod의 label 지정

```bash
$ kubectl get pods -L app # app label 추가 표시
```

### RS와 scaling

- pod 수 변경 방법 2가지

#### manifest 수정 후 `kubectl apply -f` 

```bash
$ sed -i -e 's|replicas: 3|replicas: 4|' sample-rs.yaml
$ kubectl apply -f sample-rs.yaml
```

#### `kubectl scale` 명령어 사용

```bash
kubectl scale ReplicaSet sample-rs --replicas 5
```

#### 일치성 기준 조건과 집합성 기준 조건

- replica 제어 조건
  - replication controller: equality-based selector
    - 조건부에 일치/불일치 조건 지정 (`=`,`!=`)
  - ReplicaSet: set-based selector
    - 조건부에 일치/불일치/집합 조건 지정 (`=`,`!=`, `in`, `notin`, `exists`)

## Deployment

- 여러 ReplicaSet을 관리하여 rolling update 확은 rollback 등을 구현
- 동작
  1. 신규 rs 생성
  2. 신규 replica 수를 단계적으로 늘림 (0/3 -> 1/2 -> 2/1 -> 3/0)
  3. 이전 replica 수를 단계적으로 줄임 (3/0 -> 2/1 -> 1/2 -> 0/3)
  4. 이전 replica 수를 0으로 유지

### deployment 생성

```yaml
apiVersion: v1
kind: Deployment
metadata:
  name: sample-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: sample-app
  template:
    metadata:
      labels:
        app: sample-app
    spec:
      containers:
        - name: nginx-container
          image: nginx:1.16
```

- image update
  - manifest 파일 수정 후 `kubectl apply`
  - `kubectl set image deployment sample-deployment nginx-container=nginx:1.17`

### deployment update 조건

- deployment 변경이 일어나면 ReplicaSet 생성
  - 변경
    - 예외: replica 수의 변경 등
    - 포함: 생성된 pod의 내용 변경

### 변경 rollback

```bash
$ kubectl rollout history deployment sample-deployment [--revision 1] # 변경 이력 확인
$ kubectl rollout undo deployment sample-deployment --to-revision 1 # 변경 이력 확인
```

### deployment 변경 일시 중지

```bash
$ kubectl rollout pause deployment sample-deployment # 일시 정지
$ kubectl rollout resume deployment sample-deployment # 재개
```

- `pause` 상태에서는 update 반영/rollback 진행 안 됨

### deployment update 전략

- `spec.startegy.type` 값 설정
  - default: RollingUpdate

#### Recreate

- 모든 pod 삭제 후 다시 pod 생성
- downtime 존재
- 빠름

```yaml
...
spec:
  strategy:
    type: Recreate
...
```

#### RollingUpdate

- `maxUnavailable`: 동시에 정지 가능한 최대 pod 수
  - 줄이고 생성
- `maxSurge`: 동시에 생성할 수 있는 최대 pod 수
  - 생성하고 줄임
- 둘 다 동시에 0은 불가능
- 백분률로도 지정 가능 (default: `25%`)

```yaml
...
spec:
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
      maxSurge: 1
...
```

### 상세 update parameter

- `minReadySeconds`: pod가 Ready 상태가 된 후, 다음 pod의 교체가 가능하다고 판단하기까지의 최소 시간
- `revisionHistoryLimit`
  - deployment가 유지할 ReplicaSet 수
  - rollback이 가능한 이력 수
- `progressDeadlineSeconds`
  - Recreate/RollingUpdate 처리 timeout 시간
  - timeout 경과시 자동으로 rollback

#### `spec` 아래애서 설정 가능

```yaml
...
spec:
  minReadySeconds: 0
  revisionHistoryLimit: 2
  progressDeadlineSeconds: 3600
...
```

### deployment scaling

- ReplicaSet과 같은 방법으로 scale

### manifest를 사용하지 않고 deployment 생성

- image 확인 등 간단한 작업 용으로 사용 가능

```bash
$ kubectl create deployment sample-deployment-by-cli --image nginx:1.16
```

## DaemonSet

- ReplicaSet의 특수한 형태
- 각 node에 pod을 하나씩 배치하는 resource
- Replica 수를 지정할 수 없음
- 하나의 node에 두 개 이상의 pod를 배치할 수 없음
- pod를 배치하지 않을 예외 node 선택 가능
- node가 늘어나면 자동으로 배치 => `fluentd`(로그 수집), `Datadog`(모니터링) 같은 모든 노드에서 동작해야하는 process 배치 적합

### DaemonSet 생성

```yaml
apiVersion: v1
kind: DaemonSet
metadata:
  name: sample-ds
spec:
  selector:
    matchLabels:
      app: sample-app
  template:
    metadata:
      labels:
        app: sample-app
    spec:
      containers:
        - name: nginx-container
          image: nginx:1.16
```

### DaemonSet update 전략

```yaml
...
spec:
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 2
...
```

#### OnDelete

- DaemonSet Manifest 변경 시 바로 update X
- 모종의 이유로 pod이 다시 생성될 때 적용

#### RollingUpdate

- `maxSurge` 설정 불가능
  - 동시에 동일한 pod 생성이 불가능하기 때문
- default

## StatefulSet

- ReplicaSet의 특수한 형태
- DB등과 같이 stateful한 wrokload에 사용
- 주된 차이점
  - pod명의 접미사는 숫자 index
    - pod명이 바뀌지 않음
  - 데이터 영구적으로 저장
    - Persistent Volume 사용 시, pod를 재기동할 때 같은 disk 사용

### StatefulSet 생성

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: sample-statefulset
spec:
  serviceName: sample-statefulset
  replicas: 3
  selector:
    matchLabels:
      app: sample-app
  template:
    metadata:
      labels:
        app: sample-app
    spec:
      containers:
      - name: nginx-container
        image: nginx:1.16
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 1G
```

### StatefulSet scaling

- `kubectl apply -f` 또는 `kubectl scale` 사용
  ```bash
  kubectl scale statefulset sample-statefulset --replicase=5
  ```
- pod를 동시에 하나씩만 생성/삭제하기에 오래 걸림
- index 순서로 생성, index 역순으로 삭제되기 때문에, 0번째 pod를 Master node로 사용할 수 있음

### StatefulSet의 LifeCycle

- `spec.podManagementPolicy`
  - `Parallel`로 선택하면 동시에 pod 생성 가능
  - `OrderedReady`: default

### StatefulSet update 전략

```yaml
...
spec:
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      partition: 3
...
```

#### OnDelete

#### RollingUpdate

- `maxSurge`, `maxUnavailable` 사용 불가능
- `partition`: 전체 pod중 어떤 pod을 update 할 것인지 결정
  - partition 값보다 작은 index를 가진 pod은 update 되지 않음
- default

### Persistent Volume Data 저장 확인

- `/dev/sdb` 등 별도 PV가 mount 되어있음
- pod 삭제되어도 복구 후 파일 유실되지 않음

```bash
$ kubectl exec -it sample-statefulset-0 -- touch /usr/share/nginx/html/sample.html # sample.html 생성
```

### StatefulSet 삭제와 PV 삭제

- PV는 StatefulSet이 삭제되어도 동시에 해제되지 않음
  - 백업 시간 부여

```bash
$ kubectl delete persistentvolumeclains www-sample-statefulset-{0..2} # PV 삭제
```

## Job

- 한 번만 실행되는 resource
- pod이 정상적으로 정지되는 것을 전제로 만들어짐

### Job 생성

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: sample-job
spec:
  completions: 1
  parallelism: 1
  backoffLimit: 10
  template: # selector 명시적으로 지정 가능하지만 uuid 자동 생성 되기에 별로 추천하지 않음
    spec:
      containers:
      - name: tools-container
        image: amsy9=820/tools:v2.0
        command: ["sleep"]
        args: ["60"]
      restartPolicy: Never
```

### restartPolicy에 따른 동작 차이

- `spec.template.spec.restartPolicy`

#### Never

- 장애 발생 시, 신규 pod 생성

#### OnFailure

- 쟁애 발생 시, 동일한 pod를 사용하여 job 다시 시작

### Task와 작업 queue 병렬 실행

- `completions`: m회 성공하면 종료
  - 나중에 변경할 수 없음
- `parallelism`: 동시에 n개 실행
  - 남은 성공 횟수가 n보다 작으면 남은 성공 횟수만큼 병렬로 실행
- `backoffLimit`: 실패 허용 횟수

```bash
$ kubectl patch job sample-single-workqueue-job -p '{"spec": {"completions": 2}}' # completions 변경
```

### 일정 기간 후 job 삭제

- `spec.ttlSecondsAfterFinished` 설정

```yaml
...
spec:
  ttlSecondsAfterFinished: 30
...
```

### Manifest 없이 Job 생성

- `kubectl create job`

```bash
$ kubectl create job sample-job-by-cli \
--image=amsy810/tools:v2.0 \
-- sleep 30
```

## CronJob

- scheduling 된 시간에 Job 생성

### CronJob 생성

```yaml
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: sample-cronjob
spec:
  schedule: "*/1 * * * *"
  concurrencyPolicy: Allow
  startingDeadlineSeconds: 30
  successfulJobsHistoryLimit: 5
  failedJobHistoryLimit: 3
  suspend: false
  jobTemplate:
    spec:
      completions: 1
      parallelism: 1
      backoffLimit: 0
      template:
        spec:
          containers:
          - name: tools-container
            image: amsy9=820/tools:v2.0
          restartPolicy: Never
```

### CronJob 일시 정지

- `spec.suspend` 설정

```bash
$ kubectl patch cronjob sample-cronjob -p '{"spec":{"suspend":true}}'
```

### CronJob을 임의의 시점에 실행

```bash
$ kubectl create job sample-job-from-cromjob --from cronjob/sample-cronjob # from을 이용해 cronjob 으로 job 생성
```

### 동시 실행 제어

- `spec.concurrencyPolicy`

#### Allow

- default
- 동시 실행 제한 X

#### Forbid

- 동시 실행 제한

#### Replace

- 이전 job의 실행을 취소하고 job을 시작

### 실행 시작 기한 제어

- 지연되는 경우, 허용 시간 설정
- `spec.startingDeadlineSeconds`
- default값은 무한대

### CronJob 이력

- 저장할 job 갯수 설정
- `spec.successfulJobsHistoryLimit`
- `spec.failedJobsHistoryLimit`
- 별도 log system으로 수집하면 따로 확인 불필요 + 가용성 높은 환경에 로그 저장

### Manifest를 사용하지 않고 CronJob 생성

- `kubectl create cronjob`

```bash
$ kubectl create cronjob sample-cronjob-by-cli \
--image=amsy810/random-exit:v2.0 \
--schedule "*/1 * * * *" \
--restart Never
```
